---
title: "Text Prediction"
author: "Guido Gallopyn"
date: "March 25-April26, 2015"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=FALSE)

setwd("~/Documents/Courses/DataScience/CapStone")

library(tm)
#library(RWeka) # don't load it, see http://stackoverflow.com/questions/17703553/bigrams-instead-of-single-words-in-termdocument-matrix-using-r-and-rweka
library(ggplot2)
library(Hmisc)
library(plyr)
library(dplyr)
library(tidyr)
library(slam)
library(knitr)

options(mc.cores=1) # RWeka bug workaround 

```

## Introduction

Next word prediction accuracy can be expected to be $1 / PP$, with $PP$ being the perplexity measured with a language model. As a reference point, the lowest perplexity that has been published on the Brown Corpus (1 million words of American English of varying topics and genres) is about 247 using a trigram model, or expected word prediction accuracy of 0.4%. We expect the perplexity of the Capstone Corpus to be in the same order of magnitude, and we make the observation that unless the language model domain can be constrained drastically, a language model alone will likely provide accuracy levels lower than 1%.

To build an accurate text prediction system, more information, other than the previous words, is needed to narrow down the large number of posibilities. 

With an aditional observation $o$ the prediction probability then becomes $p( w_{i} \mid w_{i-n+1}... w_{i-1} ; o )$ 

With Bayes Rule 

$p( w_{i} \mid w_{i-n+1}... w_{i-1}; o ) = \frac {p( o \mid w_{i} ) . p( w_{i} \mid w_{i-n+1}... w_{i-1} )} {p(o)}$ 

The second factor in the right hand side of this equation is exactly the language model probability, the first factor is the the probability of observing $o$ given $w_{i}$. 

Examples of $p ( o \mid w_{i} )$ may be the probability of observing a sound $o$ given the word $w_{i}$ in speech recognition, or  probability of observing a bitmap of cursive text $o$ given the word $w_{i}$ in hand writind recognition recognition, or in text prediction the prefix of a word as an observation $o$ to predict the word $w{i}$, in addition to the preceding words in the sentence.

p( o  w{i} ) =   (o, w{i}) 
 
with $\delta(o, w_{i}) = 1$ if $prefix ( w_{i}, o ) = true$ or 0 otherwise

### LM implementation requirements

1) efficiently store $p(w_i | w_{i-n+1}...w_{i-1})$ of multi dim for n-gram

2) fast retrieval of $p(w_i | w_{i-n+1}...w_{i-1})$ given $w_{i-n+1}...w_{i-1}$

3) fast retrieval of followers { $w_i$ : $p(w_i | w_{i-n+1}...w_{i-1}) > 0$ } given $w_{i-n+1}...w_{i-1}$

4) fast retrieval of followers given an observation ${ wi :  wi & p(wi | w{i-n+1}...w{i-1}) > 0  }$ given $w_{i-n+1}...w_{i-1} and o$


### LM implmentation based on Hash Tables

- storage of ( key= $w_{i-n+1}...w_{i}$ , value=$p$ or $log(p)$ ) , hashkeys is concatenated string of words

- storage and retrieval time compexity is $O(1)$, space complexity $O(|Ngrams|)$ with $|Ngrams|$ the number of N-grams to be stored

- one hash table with all N-grams, issue is retreiving the list of followers. The followers can be stored with the probability as a list of the hash-keys of the following N-grams, this provides $O(1)$ folowers retreival time complexitybut increases space complexity $O(|Ngrams| . P)$ with $P$ being the perplexity of the model. Alternatively, the list of followers can be obtained by searching the list of N-gram keys with the R `grepl` function whih  $O(|Ngrams|) < O(|V|^n)$ time complexity. This can be made faster by using a list of N hash tables, on for unigrams, bigrams, etc. This has the same space complexity $O(|Ngrams|) = O(|1grams|) + O(|2grams|) + ...$ but reduced the time complexity for bigramsto $O(|2grams|) < O(|V|^2)$  

- implemented in R via the [hash](http://cran.r-project.org/web/packages/hash/hash.pdf) package.


### How to store and process large corpora in R?  

Text corpora stored as files in a directory can be loded via the Corpus functionality in tm package. A Corpus is a collection of TextDocuments. There this a Volatile Corpus implemention (VCorpus) that loads the entire corpus in R memory and there is a Permanent Corpus implementation (PCorpus) that loads the corpus in a filehash that is kept on disk. This relies on filehash package from Roger Peng, that implements a file based hash table in R.

The Capstone corpus  is about 100 million words and can be loaded in memory on a Mac Pro with 8MByte RAM, but I found any text processing very slow. For corpora up to 10 million words, text processing works fine with tm VCorpus.

### Splitting and subsampeling data

To properly tune and evaluate performace, I split the data in a training, evaluation and test corporus, and to further speed up calculations I subsample the training corpus in large, medium and small training sets, all while preserving balanced amounts of Blogs, News and Tweets. 

Below is the R code to load a text corpus in memory, to split it into two complementary pieces given a fraction, and to subsample a text corpus, and as an example how to store the obtained small corpus.  

- Full Corpus is about 100 million words

- Testing corpus is 5%, or about 5 million words

- Evaluation corpus is 10% of 95%, or 9.5%. or about 9.5 million words

- Training corpus is 90.5% of full corpus, or about 90 million words

- Large Corpus is 10% of training corpus, or about 9 million words

- Medium Corpus is 1/3 of large corpus, or about 3 million words

- Small corpus is 1/3 of mediancorpus, or about 1 million words

``` {r splitting,eval=FALSE}

sample.fraction <- function(len, frac)  sample(len,frac*len)

subsample.TextDocument <- function(doc,frac) 
  PlainTextDocument( sample(content(doc), frac * length(content(doc))), 
                     id = meta(doc,"id") )

split.Corpus <- function(corpus,frac) {
  corpus<- tm_map(corpus, function(x) { meta(x,"sample") <- sample.fraction(length(content(x)), frac); x })
  split<- list(tm_map(corpus, function(x) PlainTextDocument(content(x)[ meta(x,"sample") ],id=meta(x,"id"))),
               tm_map(corpus, function(x) PlainTextDocument(content(x)[ -meta(x,"sample") ],id=meta(x,"id"))))
  return(split)
}
# load the text copus in memory
corpus <- VCorpus(DirSource("data/final/en_US/"))
split <- list()

# hold out data for evaluation and testing
split[c("testing","rest")]        <- split.Corpus(corpus,1/20)
split[c("evaluation","training")] <- split.Corpus(split$rest,1/10)

# subsample the training set
large   <- tm_map(split$training, subsample.TextDocument, 1/10)
medium  <- tm_map(large, subsample.TextDocument, 1/3)
small   <- tm_map(medium, subsample.TextDocument, 1/3)

writeCorpus(small, path = "data/small/en_US/")
```


### Data cleaning

``` {r cleaning,eval=FALSE}

cleanctrl <-list(content_transformer(function(x) iconv(x, from="latin1", to="ASCII", sub="")),
                 content_transformer(tolower),
                 removePunctuation,
                 removeNumbers,
                 stripWhitespace)
      
corpus <- tm_map(corpus, FUN = tm_reduce, tmFuns= cleanctrl)
```


### NGram tokenization and counting 

The RWeka NGramTokenizer is used in combination with the tm TextDocumentMatrix functionality to count ngrams in a corpus. The ngram counts are retreived with slam package row_sums that operates on sparse matrices. N-gram fequency treshold is applied with standard R subset functionality.  

``` {r counting, eval=FALSE }
tokctrl <- list(tokenize = function(x) RWeka::NGramTokenizer(x, RWeka::Weka_control(min = n, max = n)), # count
                wordLengths=c(1, Inf))

cnt <- slam::row_sums(tm::TermDocumentMatrix(corpus, control=tokctrl))    

if (threshold > 0) cnt <- cnt[ cnt > threshold ]
```


``` {r tab2}
load("../task4/tab2.RData")
kable(df)
```

## LM training

Katz backoff with Good-Turing discounting.

Making trigram backoff LM needs to be precomputed. We'll explore an LM R-S3 class, train it and use it as a predictor.
requres tm, RWeka and slam.

Small - xxx words

Medium - xxx words

K | 1Grams |  2Grams |  3Grams | 4 Grams
- | ------ | ------- | ------- | ---------
0 | 103130 | 1009382 | 2008198 | 2347551
1 |  46352 |  226357 |  185988 |   66401
2 |  33666 |  126163 |   81100 |   21129
3 |  27451 |   87257 |   48881 |   10686
5 |  20866 |   53948 |   25273 |    4394

Large - xxx WORDS


## Evaluations 

### Coverage

Coverage is the pecentage of words that can be predicted by a text predictor, and is in essence determined by the words in a text corpus that occur in the LM vocubulary. Coverage is best measured on an evaluation data set that is held out from the data set used for constructing the wordlist.

The plot below shows Coverage in function of vocabulary size of lanauge models trained on various size corpora, using different word frequency thresholds, (threshold noted in the circles). Measurements were done on a 1/10 subsample of the evaluation corpus described above (so about 1 milion words). Preprocessing, cleaning, tokenization was done as described above.

Observe that 

- word coverage is over 90% with system of about 10,000 words, and that a system of 50,000 words can push coverage to 97%

- for a given vocabulary size, vocabularies trained on the large corpus (9 million words) provides better coverage for a vocabulary trained on small corpus (1 million words) gives slightly better coverage for a given vocabulary size than LMs trained on the medium corpus.

- word frequency thresholds 7 or 10 seem reasonable on the large corpus, and are more or less equivalent to 2 or 3 with medium corpus. We expect a coverage of 96-97% with a LM with 25 to 30 thousand words.


```{r Coverage, echo=FALSE}
load("Coverage.RData")
ggplot(aes(Voc,Coverage,group=data,colour=data), data=moddf) + 
  geom_line() + 
  geom_point(size=9, shape=21) + 
  geom_text(aes(label=K), size=6)
```

### Language Model Perplexity Evaluation 

Perplexity measured with Katz LMs of order 1, 2 and 3 (upto trigrams) on a held out evaluation data set. LMs were trained on the large (9 million words) training corpus, with n-gram frequency threshold K. Voc is the resulting number of unique words in the LM.

```{r Perplexity}
load(file="perplexity.large.RData")
moddf %>% ggplot(aes(Voc,PP,group=N,colour=as.factor(N))) + 
          geom_line() +
          geom_point(size=3, shape=21) 
```

Observe that bigram LMs are much better that unigram LMs, and that trigram LMs further improve on bigram LMs. The perplexity of the Unigram increases as we increase vocabulary size, it is easier to predict <UNK> then very low frequency words, the perplexity for bigram and trigram LMs improves marginally as vocabulary increases.

Observe as well that the perplexity is very high, the best perplexity is 475 with a trigram back-off LM with about 50k words. News has a higher Perplixity than Blogs and tweets, as shown in the tqble below.

```{r Perplexitytab, results='asis', echo=FALSE}
kable(pptab)
```

### Word Prediction Accuracy Evaluation

```{r Accuracy, echo=FALSE}
load(file=file.path("../acc", "acc.summary.RData"))
accuracy %>% filter((N==3 & K==7) | (N==3 & K==20) | (N==2 & K==40)) %>%
             ggplot(aes(factor(K),Accuracy,color=factor(K))) +   
             geom_boxplot() +
             facet_grid(~ PrefixLen)

accuracy %>% filter((N==3 & K==7) | (N==3 & K==20) | (N==2 & K==40)) %>%
             group_by(K,PrefixLen) %>%
             summarise(acc=mean(Accuracy)) %>%
             spread(PrefixLen,acc)
```

### Evaluation Corpus median rank
to do

### Evaluation Corpus top5 accuracy
to do

## Conclusion

which model to use for shiny app

what is expected performance (accuracy)


## Schedule

### Week March 30

- due April 5: peer reviews milestone reports (done)

- debug Good-Turing discounting (done see task3explore4)

- perplexity calculation (done.... see task4explore1, double check trigrams)

- perplexity evaluation on training set with different model params (N,K) - done for N=1:4, K=0 

- probablity for OOV (done)

- storing, processing, tokenizing and counting very large corpora (done, change of plan ==> python) 

- tokenization and ngram countin on very large corpora in python (in progress)

- lm calculation in python (in progress, Katz to do)

- lm storage in DARPA or SRI format in python (in progress, katz to do )

- lm file reading in DARPA or SRI format in R (Done, integrated with S3 class)

- fix memory leaks due to environments and hash

- ngramlm print function (preliminary)

- due April 5: quiz 3 (preliminary script with Katz bacckoff 4gram LM,k=0, good-turing smoothing)


### Week April 6

- python lm building (abandonned)

- finalization cleaning and ngram counting of large corpora with tm, Rweka

- split data in training, evaluation and test sets

- debug Katz LM training

- training models using on small, medium and large corpus, with N=2,4 and treshold 1..5

- N-best prediction of w given history, speed optimization 

- prototype shiny app (in progress)

- lm summary (in progress)

## Week April 13

- accuracy evaluation scripts on test corpus in R (coverage, perplexity, accuracy, average rank, n-best accuracy)

- more model building. large, N=3

- lm summary

- perplexity debugging

- prediction with a prefix, update NGramLM with word trie, updated models

- accuracy evaluation scripts on test corpus in R (coverage, perplexity)

## Week April 20

- host shiny app with LM model, intial eval

- creation of test sets (y ~ x) format for accuracy evaluation

- accuracy evaluation scripts on test corpus in R (accuracy, average rank, n-best accuracy)

- final model for demonstation

- shiny app finalization (app, test, hood, about)

- presentation

- due April 26 shiny app

- due April 26 presentation

## Week April 27

- due May 3, peer evaluations final project


## Things to do

- S3 object finalization

- data tables based inplementation (read up on data tables package)

- benchmark hash versus data table implementation

- tuning parameters N, voc size (accuracy vs speed and size)

- other LMs : interpolated LMs (state of the art neural net and Recursive NN)

- other smoothing: add-k smoothing, n-1 prior discounting, absolute discounting, keyser-ney

- different tokenization with start and stop at end

- adding other corpora for news and twitter

- adding dictionaries

- creative research: class LMs

- creative research: history buffers (bag of non-stem words in the sentence )

- reading up on predictive text approaches


- download more data eg (ftp://ftp.gnu.org/gnu/aspell/dict/en/aspell6-en-7.1-0.tar.bz2)


## Done 

- subsampleing data set (100 million words) in micro (1000 words), small (1 Million), medium(3Million) and large(10 million) data sets

- tokenization and cleaning piplines based on wm and Rweka

- NgramLM R class based on hash

- Katz backof ngram training receipe

- quiz 1 (score 10/10)

- quiz 2 (score 9/10)

- milestone 1 report on R pubs

The table below shows the results of a benchmark test comparing volatile and permanent corpora for different size corpora. Measurements are corpus load time, corpus cleaning time (as in cleaning receipe above) and tokenization and word counting with tm::TermDocumentMatrix.

Corpus |  nWords | tm:VCorpus | tm:PCorpus | Python
-------|---------|------------|------------|---------------
small  |  1M     | 7.9        |  9.6       | 2.5 sec
medium |  3M     |            |            | 7 sec
large  |  10M    |            |            | 23 sec
final  | 100M    |    NA      |    ?       | 3:47 min 

==> decision: process corpoara and build lms with python, save in ARPA LM format. Read the LM file in R and use for prediction 

yet another idea
- line per line, reading processing and counting (could be exted to mapreduce), freq is stored as hash
with NLTokenizer small 1.15 mins
with WekaWordTokenizer small 1.24 mins
with WekaNGramTokenizer N=1 small 1.27 mins
with WekaNGramTokenizer N=2 small  mins
with WekaNGramTokenizer N=3 small  mins
with WekaNGramTokenizer N=4 small  mins

``` {r loading,eval=FALSE}

# read an english corpus
small <- VCorpus(DirSource("data/small/en_US/"))

# cleaning receipe
small <- tm_map(small, content_transformer(function(x) iconv(x, from="latin1", to="ASCII", sub="")))
small <- tm_map(small, content_transformer(tolower))
small <- tm_map(small, removePunctuation, preserve_intra_word_dashes = TRUE)
small <- tm_map(small, removeNumbers)
small <- tm_map(small, removeWords, readLines("data/en-US-bad-words.txt"))
small <- tm_map(small, stripWhitespace)
```

``` {r tab1,eval=FALSE}
kable(Corpus.summary(small))
```

Approach 
- split the training corpus in k=30 folds
- count Ngrams with RWeka NGram tokenizer on each fold, storing a tm TermDocumentMatrix
- combine the the K term Document Matrix using Map Reduce paradigm that fires combines  TermDocumentMatrix and sums counts per term
- count N grams seperately.

N |  nGrams | unique nGrams |   Size  | Time 
--|---------|---------------|---------|---------
1 |         |               |   7.2MB |  7 min
2 |         |               | 109.2MB | 29 min
3 |         |               | 375.2MB | 1hr41min
4 |         |               |         | 4hrs30min 

ref[1]  
Brown, Peter F. et al. (March 1992). "An Estimate of an Upper Bound for the Entropy of English". Computational Linguistics 18 (1).

