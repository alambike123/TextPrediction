---
title: "NGramLM"
author: "Guido Gallopyn"
date: "March 25, 2015"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=FALSE)
```

``` {r packages,echo=FALSE, warning=FALSE, message=FALSE}
# exploratory analysis script for Capstone task 3

setwd("~/Documents/Courses/DataScience/CapStone")

library(tm)
#library(RWeka) # don't load it, see http://stackoverflow.com/questions/17703553/bigrams-instead-of-single-words-in-termdocument-matrix-using-r-and-rweka
library(ggplot2)
library(plyr)
library(dplyr)
library(slam)
library(knitr)

options(mc.cores=1) # RWeka bug workaround 

Corpus.summary <- function(corpus) 
  ldply(lapply(corpus, function(x) { ellen <- nchar(content(x))
                                     wordlist <- RWeka::WordTokenizer(x)  
                                     data.frame(TextDocument=meta(x,"id"),
                                                NChars=sum(ellen), NElements=length(content(x)),
                                                MedianElement=median(ellen), MaxElement=max(ellen),
                                                NWords=length(wordlist), NVoc=length(unique(wordlist)))} ), 
        data.frame)
getwd()
```

``` {r loading,eval=FALSE}

# read an english corpus
#corpus <- VCorpus(DirSource("data/final/en_US/"))
small <- VCorpus(DirSource("data/small/en_US/"))
#small <- VCorpus(DirSource("data/micro/en_US/"))

# cleaning
small <- tm_map(small, content_transformer(function(x) iconv(x, from="latin1", to="ASCII", sub="")))
small <- tm_map(small, content_transformer(tolower))
small <- tm_map(small, removePunctuation, preserve_intra_word_dashes = TRUE)
small <- tm_map(small, removeNumbers)
small <- tm_map(small, removeWords, readLines("data/en-US-bad-words.txt"))
small <- tm_map(small, stripWhitespace)
```

``` {r tab1,eval=FALSE}
kable(Corpus.summary(small))
```


what is needed ?

1) efficiently store $p(w_i | w_{i-n+1}...w_{i-1})$ of multi dim for n-gram

2) fast retrieval of $p(w_i | w_{i-n+1}...w_{i-1})$ given $w_{i-n+1}...w_{i-1}$

3) fast retrieval of followers { $w_i$ : $p(w_i | w_{i-n+1}...w_{i-1}) > 0$ } given $w_{i-n+1}...w_{i-1}$

Options for storage

Approach    | Description |Strength | Weakness
----------- | -------------| -- | --
Hash Tables | | |
Sparse Matrix | | |
Tree       | | |

### Hash Table:

- storage of ( key= $w_{i-n+1}...w_{i}$ , value=$p$ or $log(p)$ ) , hashkeys is concatenated string of words

- storage and retrieval time compexity is $O(1)$, space complexity $O(|Ngrams|)$ with $|Ngrams|$ the number of N-grams to be stored

- one hash table with all N-grams, issue is retreiving the list of followers. The followers can be stored with the probability as a list of the hash-keys of the following N-grams, this provides $O(1)$ folowers retreival time complexitybut increases space complexity $O(|Ngrams| . P)$ with $P$ being the perplexity of the model. Alternatively, the list of followers can be obtained by searching the list of N-gram keys with the R `grepl` function whih  $O(|Ngrams|) < O(|V|^n)$ time complexity. This can be made faster by using a list of N hash tables, on for unigrams, bigrams, etc. This has the same space complexity $O(|Ngrams|) = O(|1grams|) + O(|2grams|) + ...$ but reduced the time complexity for bigramsto $O(|2grams|) < O(|V|^2)$  

- implemented in R via the [hash](http://cran.r-project.org/web/packages/hash/hash.pdf) package.

### Sparse matrices

- implemented in R via the `slam` package for two dimensional matrices

### trees


making trigram backoff LM needs to be precomputed we'll explore creting an LM S3 class, train it and use it as a predictor
requres tm, RWeka and slam

``` {r class,eval=FALSE}
library(hash)

NGramLM <- function(corpus, N=2) {
  model <- list( "N" = N )
  class(model) <- append(class(model),"NGramLM")
  for( n in 1:N ) model[[paste0(n,"-grams")]] <- 
                  hash(row_sums(tm::TermDocumentMatrix(corpus, 
                                                  control=list( tokenize = function(x) RWeka::NGramTokenizer(x, RWeka::Weka_control(min = n, max = n)),
                                                            wordLengths=c(1, Inf)))))

  return(model)
}


summary.NGramLM <- function(model)
{
  return(model$N)
}

predict.NGramLM <- function(model,sentence, alts=NULL)
{
  return(sample(alts,1))
}

# use
mod2 <- NGramLM(small,2)
summary(mod2)
predict(mod2, "this is a ", c("test","pipe"))

Map(function(x,y) predict(mod2,x,y), quiz, alternatives) #ugly!!! rewrite
```
