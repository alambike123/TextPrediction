---
title: "NGramLM"
author: "Guido Gallopyn"
date: "March 25, 2015"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=FALSE)
```

``` {r packages,echo=FALSE, warning=FALSE, message=FALSE}
# exploratory analysis script for Capstone task 3

setwd("~/Documents/Courses/DataScience/CapStone")

library(tm)
#library(RWeka) # don't load it, see http://stackoverflow.com/questions/17703553/bigrams-instead-of-single-words-in-termdocument-matrix-using-r-and-rweka
library(ggplot2)
library(plyr)
library(dplyr)
library(slam)
library(knitr)

options(mc.cores=1) # RWeka bug workaround 

Corpus.summary <- function(corpus) 
  ldply(lapply(corpus, function(x) { ellen <- nchar(content(x))
                                     wordlist <- RWeka::WordTokenizer(x)  
                                     data.frame(TextDocument=meta(x,"id"),
                                                NChars=sum(ellen), NElements=length(content(x)),
                                                MedianElement=median(ellen), MaxElement=max(ellen),
                                                NWords=length(wordlist), NVoc=length(unique(wordlist)))} ), 
        data.frame)
getwd()
```

``` {r loading,eval=FALSE}

# read an english corpus
#corpus <- VCorpus(DirSource("data/final/en_US/"))
small <- VCorpus(DirSource("data/small/en_US/"))
#small <- VCorpus(DirSource("data/micro/en_US/"))

# cleaning
small <- tm_map(small, content_transformer(function(x) iconv(x, from="latin1", to="ASCII", sub="")))
small <- tm_map(small, content_transformer(tolower))
small <- tm_map(small, removePunctuation, preserve_intra_word_dashes = TRUE)
small <- tm_map(small, removeNumbers)
small <- tm_map(small, removeWords, readLines("data/en-US-bad-words.txt"))
small <- tm_map(small, stripWhitespace)
```

``` {r tab1,eval=FALSE}
kable(Corpus.summary(small))
```


what is needed ?

1) efficiently store $p(w_i | w_{i-n+1}...w_{i-1})$ of multi dim for n-gram

2) fast retrieval of $p(w_i | w_{i-n+1}...w_{i-1})$ given $w_{i-n+1}...w_{i-1}$

3) fast retrieval of followers { $w_i$ : $p(w_i | w_{i-n+1}...w_{i-1}) > 0$ } given $w_{i-n+1}...w_{i-1}$

Options for storage

Approach    | Description |Strength | Weakness
----------- | -------------| -- | --
Hash Tables | | |
Sparse Matrix | | |
Tree       | | |

### Hash Table:

- storage of ( key= $w_{i-n+1}...w_{i}$ , value=$p$ or $log(p)$ ) , hashkeys is concatenated string of words

- storage and retrieval time compexity is $O(1)$, space complexity $O(|Ngrams|)$ with $|Ngrams|$ the number of N-grams to be stored

- one hash table with all N-grams, issue is retreiving the list of followers. The followers can be stored with the probability as a list of the hash-keys of the following N-grams, this provides $O(1)$ folowers retreival time complexitybut increases space complexity $O(|Ngrams| . P)$ with $P$ being the perplexity of the model. Alternatively, the list of followers can be obtained by searching the list of N-gram keys with the R `grepl` function whih  $O(|Ngrams|) < O(|V|^n)$ time complexity. This can be made faster by using a list of N hash tables, on for unigrams, bigrams, etc. This has the same space complexity $O(|Ngrams|) = O(|1grams|) + O(|2grams|) + ...$ but reduced the time complexity for bigramsto $O(|2grams|) < O(|V|^2)$  

- implemented in R via the [hash](http://cran.r-project.org/web/packages/hash/hash.pdf) package.

### Sparse matrices

- implemented in R via the `slam` package for two dimensional matrices. Not presued.

### Trees

- not persued

## LM training

Katz backoff with Good-Turing discounting.

Making trigram backoff LM needs to be precomputed. We'll explore an LM R-S3 class, train it and use it as a predictor.
requres tm, RWeka and slam.

## LM evaluations - Katz backoff on 1 million training word corpus 


N | k | # NGrams | Model Size | Training PP
--|---|----------|------------|---------------
1 | 0 | 57289    | 398kB      | 1734
2 | 0 | 438925   |            | 145.8
3 | 0 | 766002   | 31.4MB     | 18.36
4 | 0 | 843480   | 60.1MB     | 9.04

## Things to do

## Week March 30

- due April 5: peer reviews milestone reports (done)

- debug Good-Turing discounting (done see task4explore4)

- perplexity calculation (done.... see task4explore1, double check trigrams)

- perplexity evaluation on training set with different model params (N,K) - done for N=1:4, K=0 

- probablity for OOV

- ngramlm print function


- N-gram tokenization exploration, tokenizing very large corpora (100 million words) and calculating n grams

- fix memory leaks due to environments and hash




- due April 5: quiz 3 

## Week April 6

- 

## Week April 13



## Week April 20

- due April 26 shiny app

- due April 26 presentation

## Week April 27

- due May 3, peer evaluations final project


## Schedule

- N-best prediction of w given history 


- data tables based inplementation (read up on data tables package)

- benchmark hash versus data table implementation

- accuracy evaluation on test corpus

- tuning parameters N, voc size (accuracy vs speed and size)

- other LMs : interpolated LMs (state of the art neural net and Recursive NN)

- other smoothing: add-k smoothing, n-1 prior discounting, absolute discounting, keyser-ney

- different tokenization with start and stop at end

- adding other corpora for news and twitter

- adding dictionaries

- creative research: class LMs

- creative research: history buffers (bag of non-stem words in the sentence )

- reading up on predictive text approaches

- split data in training, evaluation and test sets

- download more data eg (ftp://ftp.gnu.org/gnu/aspell/dict/en/aspell6-en-7.1-0.tar.bz2)


## Done 

- subsampleing data set (100 million words) in micro (1000 words), small (1 Million), medium(3Million) and large(10 million) data sets

- tokenization and cleaning piplines based on wm and Rweka

- NgramLM R class based on hash

- Katz backof ngram training receipe

- quiz 1 (score 10/10)

- quiz 2 (score 9/10)

- milestone 1 report on R pubs
